{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training\n",
    "The purpose of this notebook is for training the LSTM RNN for price prediction of Modern Tournament Magic: The Gathering cards. This is intended as a supplement to the Main notebook, which highlights the predictions in a more intuitive way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import mpld3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, TimeDistributed, Conv3D, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from MTGDeckScraper import get_price_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "Many useful functions for converting the time series data to supervised learning format, including: \n",
    "- Differencing and MinMaxScaling the price and tournament play data\n",
    "- Creating lag_time and target prediction sequence columns\n",
    "- Creating and fitting the lstm model\n",
    "- Inverse transforming the data for comparing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    #Creates dataframe of supervised data X for n_in days and n_out days forward\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values.reshape(-1,1)\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    #Output non_diff_scaled y_vals\n",
    "    y_raw = series_to_supervised(raw_values,n_lag,n_seq)\n",
    "    y_raw_values = y_raw.values\n",
    "    # split into train and test sets\n",
    "    y_train_raw, y_test_raw = y_raw_values[0:-n_test], y_raw_values[-n_test:]\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test,y_train_raw,y_test_raw\n",
    "\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    for j in range(1,len(yhat)+1):\n",
    "        yhat[0][j] += yhat[0][j-1]\n",
    "    ans = yhat+history\n",
    "    return ans\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons,test):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag*2], train[:, n_lag*2:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X_val, y_val = test[:, 0:n_lag*2], test[:, n_lag*2:]\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),\n",
    "                   kernel_initializer='random_uniform', return_sequences=True, stateful=True))\n",
    "    #model.add(Conv1D(filters = 32, kernel_size = [5], padding='same'))\n",
    "    model.add(LSTM(n_neurons, stateful=True, return_sequences = True))\n",
    "    model.add(LSTM(n_neurons))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=['mean_squared_error'], optimizer='adam')\n",
    "    # fit network\n",
    "    histories = []\n",
    "    for i in range(nb_epoch):\n",
    "        h_m = model.fit(X, y, epochs=1, batch_size=n_batch, validation_data = (X_val,y_val), verbose=True, shuffle=False)\n",
    "        histories.append(h_m)\n",
    "        model.reset_states()\n",
    "    return model, histories\n",
    "\n",
    "#Forecast to test values\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=1)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag*2], test[i, n_lag*2:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        # store\n",
    "        undiffed = inverse_difference(series[i],inv_scale)\n",
    "        inverted.append(undiffed)\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Loads the price and tournament play data from previously generated csv for all cards in the dataset. From this data, tournament play frequencies are generated and converted to supervised learning data. The most recent 400 days of data are taken off and used as validation data for fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Data from CSV's\n",
    "# cards = [['Shadowmoor','Fulminator Mage'], ['New Phyrexia', 'Surgical Extraction'], \n",
    "#          ['Future Sight', 'Tarmogoyf'], ['Zendikar','Scalding Tarn'], \n",
    "#          ['Innistrad', 'Snapcaster Mage'], ['Time Spiral','Ancestral Vision'], \n",
    "#          ['Mirrodin','Chalice of the Void'],['Ninth Edition', 'Blood Moon'],\n",
    "#          ['Khans of Tarkir', 'Polluted Delta'], ['Mirrodin Besieged', 'Inkmoth Nexus'],\n",
    "#          ['Eighth Edition', 'Ensnaring Bridge'], ['New Phyrexia', 'Karn Liberated'],\n",
    "#          ['Ravnica City of Guilds', 'Sacred Foundry'], ['Magic 2011', 'Leyline of the Void'],\n",
    "#          ['Avacyn Restored', 'Cavern of Souls'], ['Ravnica City of Guilds', 'Chord of Calling']]\n",
    "CopiesPlayed = pd.read_csv('Copies_Played_Culled.csv')\n",
    "CopiesPlayed['Date'] = pd.to_datetime(CopiesPlayed['Date'])\n",
    "CopiesPlayed = CopiesPlayed.set_index('Date')\n",
    "Resamp = CopiesPlayed.resample('D').sum()\n",
    "\n",
    "Prices = pd.read_csv('All_Prices.csv')\n",
    "Prices['Date'] = pd.to_datetime(Prices['Date'])\n",
    "\n",
    "cards = list(Prices.keys()[1:])\n",
    "bad_cards = ['Urza\\'s Tower']\n",
    "for baddie in bad_cards:\n",
    "    cards.remove(baddie)\n",
    "\n",
    "All_Data = []\n",
    "del(FullTrain)\n",
    "\n",
    "n_lag = 30\n",
    "n_seq = 7\n",
    "n_test = 400\n",
    "val_card = 2\n",
    "\n",
    "for card in cards:\n",
    "    Resamp['Meta_Share'] = Resamp[card]/Resamp['Total']\n",
    "    Price = Prices[['Date',card]]\n",
    "    Price = Price.rename(columns = {card:'Price'})\n",
    "    Price = Price.fillna(method='ffill')\n",
    "    Price = Price.dropna()\n",
    "    Resamp['Date'] = Resamp.index\n",
    "    df_test = Price.merge(Resamp[[card,'Meta_Share']],how='outer', left_on = 'Date', right_on = 'Date')\n",
    "    df_test = df_test.sort_values(by='Date')\n",
    "    df_test = df_test.fillna(method='ffill')\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_test['Meta_Share'] = df_test['Meta_Share']\n",
    "    df_test = df_test.fillna(method='ffill')\n",
    "    df_test['Meta_Share'] = df_test['Meta_Share'].replace(0,method='ffill')\n",
    "    df_test['Rolling_Meta'] = df_test['Meta_Share'].rolling(window = 7).mean()\n",
    "\n",
    "    dataset_p = df_test['Price']\n",
    "    dataset_m = df_test['Meta_Share']\n",
    "    scaler_p,train_p, test_p, train_p_raw, test_p_raw = prepare_data(dataset_p,n_test, n_lag, n_seq)\n",
    "    scaler_m,train_m, test_m,train_m_raw,test_m_raw = prepare_data(dataset_m,n_test,n_lag,n_seq)\n",
    "\n",
    "    train = np.concatenate((train_p[:,:n_lag],train_m[:,:n_lag],train_p[:,n_lag:]),axis=1)\n",
    "    test = np.concatenate((test_p[:,:n_lag],test_m[:,:n_lag],test_p[:,n_lag:]),axis=1)\n",
    "    \n",
    "    info_dict = {'card':card, 'train':train, 'test':test, \n",
    "                 'dataset_p':dataset_p, 'dataset_m':dataset_m, \n",
    "                 'scaler_p':scaler_p, 'scaler_m':scaler_m,\n",
    "                'test_p_raw':test_p_raw, 'test_m_raw':test_m_raw}\n",
    "    All_Data.append(info_dict)\n",
    "    try:\n",
    "        FullTrain = np.concatenate((FullTrain,train), axis=0)\n",
    "        FullTest = np.concatenate((FullTest,test), axis=0)\n",
    "    except:\n",
    "        FullTrain = train\n",
    "        FullTest = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 200\n",
    "n_epochs = 50\n",
    "n_neurons = 60\n",
    "\n",
    "\n",
    "FullTrain = FullTrain[-(len(FullTrain) - len(FullTrain)%n_batch):] #Stateful networks need samples to be divisible by batch size\n",
    "FullTest = FullTest[-(len(FullTest) - len(FullTest)%n_batch):] #Stateful networks need samples to be divisible by batch size\n",
    "\n",
    "model,histories = fit_lstm(FullTrain,n_lag,n_seq,n_batch, n_epochs,n_neurons,FullTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Only Metashare/price\n",
    "n_lag = 30\n",
    "n_seq = 7\n",
    "n_test = int(len(dataset_p)/6)\n",
    "n_batch = 1\n",
    "n_epochs = 50\n",
    "n_neurons = 4\n",
    "\n",
    "#Generate Model\n",
    "scaler_p,train_p, test_p, train_p_raw,test_p_raw = prepare_data(dataset_p,n_test, n_lag, n_seq)\n",
    "scaler_m,train_m, test_m,train_m_raw,test_m_raw = prepare_data(dataset_m,n_test,n_lag,n_seq)\n",
    "#forecasts = make_forecasts(model,n_batch,train,test,n_lag,n_seq)\n",
    "train = np.concatenate((train_m[:,:n_lag],train_p[:,n_lag:]),axis=1)\n",
    "test = np.concatenate((test_m[:,:n_lag],test_p[:,n_lag:]),axis=1)\n",
    "#model,histories = fit_lstm(train,n_lag//2,n_seq,n_batch, n_epochs,n_neurons,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Losses\n",
    "Plots the training and validation dataset losses for each training epoch for diagnosing possible overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_l = [j.history['val_loss'][0]/2 for j in histories]\n",
    "t_l = [j.history['loss'][0]*5 for j in histories]\n",
    "\n",
    "plt.plot(t_l)\n",
    "plt.plot(v_l)\n",
    "print(v_l.index(min(v_l)))\n",
    "print(min(t_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Train_on_All.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Model\n",
    "Here, the model is re-initialized from the saved weights as a work around issues with batching. By default, LSTM can only predict in batches that are identical to the training batch size. Here the model is reloaded with a \"training\" batch size of 1, but with the same weights that were saved in the previous cell. This allows us to predict one entry at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(1, 1, n_lag*2),\n",
    "               kernel_initializer='random_uniform', return_sequences=True, stateful=True))\n",
    "#model.add(Conv1D(filters = 32, kernel_size = [5], padding='same'))\n",
    "model.add(LSTM(n_neurons-1, stateful=True))\n",
    "#model.add(LSTM(n_neurons))\n",
    "#model.add(Dropout(0.4))\n",
    "model.add(Dense(n_seq))\n",
    "model.compile(loss=['mean_squared_error'], optimizer='adam')\n",
    "model.load_weights('Train_on_16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Comparisons\n",
    "Choose a card in the data set. These cells will output the RMSE for that particular card over the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_card = cards.index('Tarmogoyf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Ahead: 1, RMSE (USD): 0.9966643178047749\n",
      "Days Ahead: 2, RMSE (USD): 1.1575291615746464\n",
      "Days Ahead: 3, RMSE (USD): 1.3063169534740924\n",
      "Days Ahead: 4, RMSE (USD): 1.4314239537328344\n",
      "Days Ahead: 5, RMSE (USD): 1.5460610916649307\n",
      "Days Ahead: 6, RMSE (USD): 1.6317380887932937\n",
      "Days Ahead: 7, RMSE (USD): 1.7302835850286142\n"
     ]
    }
   ],
   "source": [
    "test_forecasts = make_forecasts(model,n_batch,train,All_Data[val_card]['test'],n_lag,n_seq)\n",
    "#train_forecasts = make_forecasts(model,n_batch,train,train,n_lag,n_seq)\n",
    "\n",
    "test_predictions = inverse_transform(np.array(All_Data[val_card]['test_p_raw'][:,-n_seq-1]),test_forecasts,scaler_m)\n",
    "\n",
    "predictions = test_predictions\n",
    "t_p = All_Data[val_card]['test_p_raw'][:,-n_seq:]\n",
    "for i in range(0,len(predictions[0][0])):\n",
    "    p = np.array([j[0][i] for j in predictions])\n",
    "    t = np.array([j[i] for j in t_p])\n",
    "    print('Days Ahead: ' + str(i+1) + ', RMSE (USD): ' + str(np.sqrt(np.mean((p-t)**2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Comparison\n",
    "The model that I seemed to perform best in terms of RMSE was a linear ridge regression on the pricing data. Here we train a ridge regressor and can compare the RMSE of the two models. For most cards, the LSTM NN provides 5-12% reduction in the RMSE for most prediction windows \n",
    "\n",
    "Occasionally there is little difference bewteen the two models. This may be due to modern tournament play not being a primary driver of demand in the validation window for that particular card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Ahead: 1, RMSE (USD): 0.9717002946090008\n",
      "Days Ahead: 2, RMSE (USD): 1.15925736970947\n",
      "Days Ahead: 3, RMSE (USD): 1.3325487732983992\n",
      "Days Ahead: 4, RMSE (USD): 1.4801430471722685\n",
      "Days Ahead: 5, RMSE (USD): 1.6137028092512138\n",
      "Days Ahead: 6, RMSE (USD): 1.7075421005911933\n",
      "Days Ahead: 7, RMSE (USD): 1.8024147733603206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Training a Ridge regressor and pr\n",
    "def prepare_lin_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values.reshape(-1,1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(raw_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return train, test\n",
    "\n",
    "train_r, test_r = prepare_lin_data(All_Data[val_card]['dataset_p'],n_test, n_lag, n_seq)\n",
    "\n",
    "# reshape training into [samples, timesteps, features]\n",
    "X, y = train_r[:, 0:n_lag], train_r[:, n_lag:]\n",
    "X_val, y_val = test_r[:, 0:n_lag], test_r[:, n_lag:]\n",
    "X = X.reshape(X.shape[0], X.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])\n",
    "# \n",
    "Ridgemodel = Ridge(alpha=.1)\n",
    "Ridgemodel.fit(X,y.reshape(-1,n_seq))\n",
    "RidgetestPredict = np.array([i for i in Ridgemodel.predict(test_r[:,:n_lag])])\n",
    "RidgetrainPredict = np.array([i for i in Ridgemodel.predict(train_r[:,:n_lag])])\n",
    "\n",
    "predictions = RidgetestPredict\n",
    "t_p = test_r[:,-n_seq:]\n",
    "for i in range(0,len(predictions[0])):\n",
    "    p = np.array([j[i] for j in predictions])\n",
    "    t = np.array([j[i] for j in t_p])\n",
    "    print('Days Ahead: ' + str(i+1) + ', RMSE (USD): ' + str(np.sqrt(np.mean((p-t)**2))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
