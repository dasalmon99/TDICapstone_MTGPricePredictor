{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import mpld3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, TimeDistributed, Conv3D, MaxPooling1D, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from MTGDeckScraper import get_price_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions\n",
    "Many useful functions for converting the time series data to supervised learning format, including: \n",
    "- Differencing and MinMaxScaling the price and tournament play data\n",
    "- Creating lag_time and target prediction sequence columns\n",
    "- Creating and fitting the lstm model\n",
    "- Inverse transforming the data for comparing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    #Creates dataframe of supervised data X for n_in days and n_out days forward\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values.reshape(-1,1)\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    #Output non_diff_scaled y_vals\n",
    "    y_raw = series_to_supervised(raw_values,n_lag,n_seq)\n",
    "    y_raw_values = y_raw.values\n",
    "    # split into train and test sets\n",
    "    y_train_raw, y_test_raw = y_raw_values[0:-n_test], y_raw_values[-n_test:]\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test,y_train_raw,y_test_raw\n",
    "\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    for j in range(1,len(yhat)+1):\n",
    "        yhat[0][j] += yhat[0][j-1]\n",
    "    ans = yhat+history\n",
    "    return ans\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons,test):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag*2], train[:, n_lag*2:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    X_val, y_val = test[:, 0:n_lag*2], test[:, n_lag*2:]\n",
    "    X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]),\n",
    "                   kernel_initializer='random_uniform', return_sequences=True, stateful=True))\n",
    "    #model.add(Conv1D(filters = 32, kernel_size = [5], padding='same'))\n",
    "    model.add(LSTM(n_neurons, stateful=True, return_sequences = True))\n",
    "    model.add(LSTM(n_neurons))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=['mean_squared_error'], optimizer='adam')\n",
    "    # fit network\n",
    "    histories = []\n",
    "    for i in range(nb_epoch):\n",
    "        h_m = model.fit(X, y, epochs=1, batch_size=n_batch, validation_data = (X_val,y_val), verbose=True, shuffle=False)\n",
    "        histories.append(h_m)\n",
    "        model.reset_states()\n",
    "    return model, histories\n",
    "\n",
    "#Forecast to test values\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=1)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag*2], test[i, n_lag*2:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        # store\n",
    "        undiffed = inverse_difference(series[i],inv_scale)\n",
    "        inverted.append(undiffed)\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Loads the price and tournament play data from previously generated csv for all cards in the dataset. From this data, tournament play frequencies are generated and converted to supervised learning data. The most recent 400 days of data are taken off and used as validation data for fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Data from CSV's\n",
    "# cards = [['Shadowmoor','Fulminator Mage'], ['New Phyrexia', 'Surgical Extraction'], \n",
    "#          ['Future Sight', 'Tarmogoyf'], ['Zendikar','Scalding Tarn'], \n",
    "#          ['Innistrad', 'Snapcaster Mage'], ['Time Spiral','Ancestral Vision'], \n",
    "#          ['Mirrodin','Chalice of the Void'],['Ninth Edition', 'Blood Moon'],\n",
    "#          ['Khans of Tarkir', 'Polluted Delta'], ['Mirrodin Besieged', 'Inkmoth Nexus'],\n",
    "#          ['Eighth Edition', 'Ensnaring Bridge'], ['New Phyrexia', 'Karn Liberated'],\n",
    "#          ['Ravnica City of Guilds', 'Sacred Foundry'], ['Magic 2011', 'Leyline of the Void'],\n",
    "#          ['Avacyn Restored', 'Cavern of Souls'], ['Ravnica City of Guilds', 'Chord of Calling']]\n",
    "CopiesPlayed = pd.read_csv('Copies_Played_Culled.csv')\n",
    "CopiesPlayed['Date'] = pd.to_datetime(CopiesPlayed['Date'])\n",
    "CopiesPlayed = CopiesPlayed.set_index('Date')\n",
    "Resamp = CopiesPlayed.resample('D').sum()\n",
    "\n",
    "Prices = pd.read_csv('All_Prices.csv')\n",
    "Prices['Date'] = pd.to_datetime(Prices['Date'])\n",
    "\n",
    "cards = list(Prices.keys()[1:])\n",
    "bad_cards = ['Urza\\'s Tower']\n",
    "for baddie in bad_cards:\n",
    "    cards.remove(baddie)\n",
    "\n",
    "All_Data = []\n",
    "del(FullTrain)\n",
    "\n",
    "n_lag = 30\n",
    "n_seq = 7\n",
    "n_test = 400\n",
    "val_card = 2\n",
    "\n",
    "for card in cards:\n",
    "    Resamp['Meta_Share'] = Resamp[card]/Resamp['Total']\n",
    "    Price = Prices[['Date',card]]\n",
    "    Price = Price.rename(columns = {card:'Price'})\n",
    "    Price = Price.fillna(method='ffill')\n",
    "    Price = Price.dropna()\n",
    "    Resamp['Date'] = Resamp.index\n",
    "    df_test = Price.merge(Resamp[[card,'Meta_Share']],how='outer', left_on = 'Date', right_on = 'Date')\n",
    "    df_test = df_test.sort_values(by='Date')\n",
    "    df_test = df_test.fillna(method='ffill')\n",
    "    df_test.dropna(inplace=True)\n",
    "    df_test['Meta_Share'] = df_test['Meta_Share']\n",
    "    df_test = df_test.fillna(method='ffill')\n",
    "    df_test['Meta_Share'] = df_test['Meta_Share'].replace(0,method='ffill')\n",
    "    df_test['Rolling_Meta'] = df_test['Meta_Share'].rolling(window = 7).mean()\n",
    "\n",
    "    dataset_p = df_test['Price']\n",
    "    dataset_m = df_test['Meta_Share']\n",
    "    scaler_p,train_p, test_p, train_p_raw, test_p_raw = prepare_data(dataset_p,n_test, n_lag, n_seq)\n",
    "    scaler_m,train_m, test_m,train_m_raw,test_m_raw = prepare_data(dataset_m,n_test,n_lag,n_seq)\n",
    "\n",
    "    train = np.concatenate((train_p[:,:n_lag],train_m[:,:n_lag],train_p[:,n_lag:]),axis=1)\n",
    "    test = np.concatenate((test_p[:,:n_lag],test_m[:,:n_lag],test_p[:,n_lag:]),axis=1)\n",
    "    \n",
    "    info_dict = {'card':card, 'train':train, 'test':test, \n",
    "                 'dataset_p':dataset_p, 'dataset_m':dataset_m, \n",
    "                 'scaler_p':scaler_p, 'scaler_m':scaler_m,\n",
    "                'test_p_raw':test_p_raw, 'test_m_raw':test_m_raw}\n",
    "    All_Data.append(info_dict)\n",
    "    try:\n",
    "        FullTrain = np.concatenate((FullTrain,train), axis=0)\n",
    "        FullTest = np.concatenate((FullTest,test), axis=0)\n",
    "    except:\n",
    "        FullTrain = train\n",
    "        FullTest = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 45s 77us/step - loss: 0.0172 - val_loss: 0.0097\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 36us/step - loss: 0.0055 - val_loss: 0.0119\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0073 - val_loss: 0.0156\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 25s 42us/step - loss: 0.0061 - val_loss: 0.0107\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 33s 56us/step - loss: 0.0053 - val_loss: 0.0121\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 35s 59us/step - loss: 0.0045 - val_loss: 0.0086\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 27s 47us/step - loss: 0.0043 - val_loss: 0.0090\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 40us/step - loss: 0.0042 - val_loss: 0.0093\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0045 - val_loss: 0.0103\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0043 - val_loss: 0.0088\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 29s 49us/step - loss: 0.0044 - val_loss: 0.0087\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 35s 59us/step - loss: 0.0046 - val_loss: 0.0089\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 32s 54us/step - loss: 0.0042 - val_loss: 0.0075\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 38us/step - loss: 0.0044 - val_loss: 0.0078\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0044 - val_loss: 0.0073\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0041 - val_loss: 0.0073\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0041 - val_loss: 0.0070\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 36us/step - loss: 0.0041 - val_loss: 0.0070\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0040 - val_loss: 0.0072\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0040 - val_loss: 0.0064\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0040 - val_loss: 0.0064\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0040 - val_loss: 0.0078\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 20s 34us/step - loss: 0.0039 - val_loss: 0.0069\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0043 - val_loss: 0.0084\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0043 - val_loss: 0.0071\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 22s 37us/step - loss: 0.0042 - val_loss: 0.0079\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 37us/step - loss: 0.0040 - val_loss: 0.0074\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 20s 34us/step - loss: 0.0038 - val_loss: 0.0081\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 36us/step - loss: 0.0040 - val_loss: 0.0067\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 35us/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 27s 45us/step - loss: 0.0038 - val_loss: 0.0068\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 33s 57us/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 34s 58us/step - loss: 0.0037 - val_loss: 0.0062\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0038 - val_loss: 0.0070\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 40us/step - loss: 0.0038 - val_loss: 0.0064\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 28s 48us/step - loss: 0.0039 - val_loss: 0.0073\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 35s 60us/step - loss: 0.0040 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 34s 57us/step - loss: 0.0038 - val_loss: 0.0078\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 24s 41us/step - loss: 0.0038 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 40us/step - loss: 0.0038 - val_loss: 0.0073\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 39us/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 21s 36us/step - loss: 0.0038 - val_loss: 0.0065\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 23s 40us/step - loss: 0.0037 - val_loss: 0.0064\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 36s 61us/step - loss: 0.0037 - val_loss: 0.0069\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 33s 57us/step - loss: 0.0039 - val_loss: 0.0081\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 29s 50us/step - loss: 0.0039 - val_loss: 0.0066\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 25s 43us/step - loss: 0.0039 - val_loss: 0.0069\n",
      "Train on 588000 samples, validate on 131400 samples\n",
      "Epoch 1/1\n",
      "588000/588000 [==============================] - 36s 62us/step - loss: 0.0038 - val_loss: 0.0074\n"
     ]
    }
   ],
   "source": [
    "n_batch = 200\n",
    "n_epochs = 50\n",
    "n_neurons = 60\n",
    "\n",
    "\n",
    "FullTrain = FullTrain[-(len(FullTrain) - len(FullTrain)%n_batch):] #Stateful networks need samples to be divisible by batch size\n",
    "FullTest = FullTest[-(len(FullTest) - len(FullTest)%n_batch):] #Stateful networks need samples to be divisible by batch size\n",
    "\n",
    "model,histories = fit_lstm(FullTrain,n_lag,n_seq,n_batch, n_epochs,n_neurons,FullTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/data3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Only Metashare/price\n",
    "n_lag = 30\n",
    "n_seq = 7\n",
    "n_test = int(len(dataset_p)/6)\n",
    "n_batch = 1\n",
    "n_epochs = 50\n",
    "n_neurons = 4\n",
    "\n",
    "#Generate Model\n",
    "scaler_p,train_p, test_p, train_p_raw,test_p_raw = prepare_data(dataset_p,n_test, n_lag, n_seq)\n",
    "scaler_m,train_m, test_m,train_m_raw,test_m_raw = prepare_data(dataset_m,n_test,n_lag,n_seq)\n",
    "#forecasts = make_forecasts(model,n_batch,train,test,n_lag,n_seq)\n",
    "train = np.concatenate((train_m[:,:n_lag],train_p[:,n_lag:]),axis=1)\n",
    "test = np.concatenate((test_m[:,:n_lag],test_p[:,n_lag:]),axis=1)\n",
    "#model,histories = fit_lstm(train,n_lag//2,n_seq,n_batch, n_epochs,n_neurons,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Losses\n",
    "Plots the training and validation dataset losses for each training epoch for diagnosing possible overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_l = [j.history['val_loss'][0]/2 for j in histories]\n",
    "t_l = [j.history['loss'][0]*5 for j in histories]\n",
    "\n",
    "plt.plot(t_l)\n",
    "plt.plot(v_l)\n",
    "print(v_l.index(min(v_l)))\n",
    "print(min(t_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Train_on_16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Model\n",
    "Here, the model is re-initialized from the saved weights as a work around issues with batching. By default, LSTM can only predict in batches that are identical to the training batch size. Here the model is reloaded with a \"training\" batch size of 1, but with the same weights that were saved in the previous cell. This allows us to predict one entry at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(1, 1, n_lag*2),\n",
    "               kernel_initializer='random_uniform', return_sequences=True, stateful=True))\n",
    "#model.add(Conv1D(filters = 32, kernel_size = [5], padding='same'))\n",
    "model.add(LSTM(n_neurons-1, stateful=True))\n",
    "#model.add(LSTM(n_neurons))\n",
    "#model.add(Dropout(0.4))\n",
    "model.add(Dense(n_seq))\n",
    "model.compile(loss=['mean_squared_error'], optimizer='adam')\n",
    "model.load_weights('Train_on_16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "val_card = cards.index('Tarmogoyf')\n",
    "test_forecasts = make_forecasts(model,n_batch,train,All_Data[val_card]['test'],n_lag,n_seq)\n",
    "#train_forecasts = make_forecasts(model,n_batch,train,train,n_lag,n_seq)\n",
    "print(len(test_forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "test_predictions = inverse_transform(np.array(All_Data[val_card]['test_p_raw'][:,-n_seq-1]),test_forecasts,scaler_m)\n",
    "print(len(test_predictions))\n",
    "print(len(test_p_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9966637637246639\n",
      "1.1575297593318543\n",
      "1.3063169488774868\n",
      "1.4314241083763224\n",
      "1.5460610996247452\n",
      "1.6317381317710828\n",
      "1.7302836016263652\n"
     ]
    }
   ],
   "source": [
    "predictions = test_predictions\n",
    "t_p = All_Data[val_card]['test_p_raw'][:,-n_seq:]\n",
    "for i in range(0,len(predictions[0][0])):\n",
    "    p = np.array([j[0][i] for j in predictions])\n",
    "    t = np.array([j[i] for j in t_p])\n",
    "    print(np.sqrt(np.mean((p-t)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'card': 'Tarmogoyf',\n",
       " 'train': array([[-0.00881242,  0.16240034, -0.05665128, ..., -0.01468737,\n",
       "         -0.02475871, -0.00629459],\n",
       "        [ 0.16240034, -0.05665128, -0.01888376, ..., -0.02475871,\n",
       "         -0.00629459, -0.00881242],\n",
       "        [-0.05665128, -0.01888376, -0.20016786, ..., -0.00629459,\n",
       "         -0.00881242, -0.00881242],\n",
       "        ...,\n",
       "        [-0.0675619 , -0.05077633,  0.03315149, ..., -0.1212757 ,\n",
       "          0.05916911, -0.00881242],\n",
       "        [-0.05077633,  0.03315149, -0.00881242, ...,  0.05916911,\n",
       "         -0.00881242,  0.03483005],\n",
       "        [ 0.03315149, -0.00881242, -0.02979438, ..., -0.00881242,\n",
       "          0.03483005, -0.00881242]]),\n",
       " 'test': array([[-0.00881242, -0.02979438, -0.02979438, ...,  0.03483005,\n",
       "         -0.00881242, -0.00797314],\n",
       "        [-0.02979438, -0.02979438, -0.01133026, ..., -0.00881242,\n",
       "         -0.00797314, -0.0096517 ],\n",
       "        [-0.02979438, -0.01133026, -0.00713386, ..., -0.00797314,\n",
       "         -0.0096517 , -0.01049098],\n",
       "        ...,\n",
       "        [-0.09022241,  0.0289551 , -0.04657994, ..., -0.00881242,\n",
       "         -0.00881242, -0.01888376],\n",
       "        [ 0.0289551 , -0.04657994,  0.45614771, ..., -0.00881242,\n",
       "         -0.01888376, -0.07931179],\n",
       "        [-0.04657994,  0.45614771, -0.04657994, ..., -0.01888376,\n",
       "         -0.07931179, -0.08015107]]),\n",
       " 'dataset_p': 292     109.50\n",
       " 293     109.50\n",
       " 294     111.54\n",
       " 295     110.97\n",
       " 296     110.85\n",
       " 297     108.57\n",
       " 298     104.06\n",
       " 299     103.54\n",
       " 300     103.24\n",
       " 301     103.28\n",
       " 302     102.86\n",
       " 303     102.97\n",
       " 304     102.97\n",
       " 305     102.97\n",
       " 306     102.06\n",
       " 307     100.94\n",
       " 308     102.05\n",
       " 309     102.01\n",
       " 310     102.01\n",
       " 311     102.05\n",
       " 312     102.59\n",
       " 313     102.57\n",
       " 314     102.37\n",
       " 315     102.37\n",
       " 316     102.28\n",
       " 317     102.44\n",
       " 318     102.99\n",
       " 319     102.70\n",
       " 320     102.81\n",
       " 321     102.34\n",
       "          ...  \n",
       " 3162     79.99\n",
       " 3163     79.99\n",
       " 3164     79.99\n",
       " 3165     79.99\n",
       " 3166     79.99\n",
       " 3167     79.99\n",
       " 3168     81.69\n",
       " 3169     80.85\n",
       " 3170     80.85\n",
       " 3171     80.85\n",
       " 3172     80.85\n",
       " 3173     81.69\n",
       " 3174     80.85\n",
       " 3175     80.85\n",
       " 3176     81.81\n",
       " 3177     81.81\n",
       " 3178     81.81\n",
       " 3179     81.81\n",
       " 3180     82.01\n",
       " 3181     81.93\n",
       " 3182     81.81\n",
       " 3183     81.69\n",
       " 3184     81.93\n",
       " 3185     81.93\n",
       " 3186     81.81\n",
       " 3187     81.81\n",
       " 3188     81.81\n",
       " 3189     81.69\n",
       " 3190     80.85\n",
       " 3191     80.00\n",
       " Name: Price, Length: 2913, dtype: float64,\n",
       " 'dataset_m': 292     0.009231\n",
       " 293     0.010665\n",
       " 294     0.008888\n",
       " 295     0.007619\n",
       " 296     0.007154\n",
       " 297     0.007843\n",
       " 298     0.009297\n",
       " 299     0.009078\n",
       " 300     0.011313\n",
       " 301     0.009615\n",
       " 302     0.009038\n",
       " 303     0.012906\n",
       " 304     0.013148\n",
       " 305     0.012186\n",
       " 306     0.015439\n",
       " 307     0.012026\n",
       " 308     0.016552\n",
       " 309     0.025758\n",
       " 310     0.017455\n",
       " 311     0.013333\n",
       " 312     0.012029\n",
       " 313     0.016296\n",
       " 314     0.004324\n",
       " 315     0.010909\n",
       " 316     0.007356\n",
       " 317     0.014141\n",
       " 318     0.015104\n",
       " 319     0.021538\n",
       " 320     0.008869\n",
       " 321     0.012500\n",
       "           ...   \n",
       " 3162    0.005362\n",
       " 3163    0.005362\n",
       " 3164    0.005362\n",
       " 3165    0.002711\n",
       " 3166    0.006667\n",
       " 3167    0.004289\n",
       " 3168    0.004289\n",
       " 3169    0.005623\n",
       " 3170    0.005623\n",
       " 3171    0.005623\n",
       " 3172    0.005333\n",
       " 3173    0.001667\n",
       " 3174    0.004671\n",
       " 3175    0.004671\n",
       " 3176    0.005516\n",
       " 3177    0.005516\n",
       " 3178    0.005516\n",
       " 3179    0.003505\n",
       " 3180    0.005333\n",
       " 3181    0.004674\n",
       " 3182    0.004674\n",
       " 3183    0.001934\n",
       " 3184    0.001934\n",
       " 3185    0.001934\n",
       " 3186    0.001974\n",
       " 3187    0.004000\n",
       " 3188    0.004000\n",
       " 3189    0.004000\n",
       " 3190    0.004000\n",
       " 3191    0.004000\n",
       " Name: Meta_Share, Length: 2913, dtype: float64,\n",
       " 'scaler_p': MinMaxScaler(copy=True, feature_range=(-1, 1)),\n",
       " 'scaler_m': MinMaxScaler(copy=True, feature_range=(-1, 1)),\n",
       " 'test_p_raw': array([[104.99, 104.74, 104.49, ..., 104.48, 104.48, 104.49],\n",
       "        [104.74, 104.49, 104.46, ..., 104.48, 104.49, 104.48],\n",
       "        [104.49, 104.46, 104.48, ..., 104.49, 104.48, 104.46],\n",
       "        ...,\n",
       "        [ 73.45,  73.9 ,  73.45, ...,  81.81,  81.81,  81.69],\n",
       "        [ 73.9 ,  73.45,  78.99, ...,  81.81,  81.69,  80.85],\n",
       "        [ 73.45,  78.99,  78.54, ...,  81.69,  80.85,  80.  ]]),\n",
       " 'test_m_raw': array([[0.00259825, 0.00259825, 0.00259825, ..., 0.00333333, 0.00543774,\n",
       "         0.00543774],\n",
       "        [0.00259825, 0.00259825, 0.00259825, ..., 0.00543774, 0.00543774,\n",
       "         0.00543774],\n",
       "        [0.00259825, 0.00259825, 0.00166667, ..., 0.00543774, 0.00543774,\n",
       "         0.004     ],\n",
       "        ...,\n",
       "        [0.005     , 0.01333333, 0.0062487 , ..., 0.004     , 0.004     ,\n",
       "         0.004     ],\n",
       "        [0.01333333, 0.0062487 , 0.0062487 , ..., 0.004     , 0.004     ,\n",
       "         0.004     ],\n",
       "        [0.0062487 , 0.0062487 , 0.0062487 , ..., 0.004     , 0.004     ,\n",
       "         0.004     ]])}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Data[val_card]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def prepare_lin_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values.reshape(-1,1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(raw_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return train, test\n",
    "\n",
    "train_r, test_r = prepare_lin_data(All_Data[val_card]['dataset_p'],n_test, n_lag, n_seq)\n",
    "\n",
    "# reshape training into [samples, timesteps, features]\n",
    "X, y = train_r[:, 0:n_lag], train_r[:, n_lag:]\n",
    "X_val, y_val = test_r[:, 0:n_lag], test_r[:, n_lag:]\n",
    "X = X.reshape(X.shape[0], X.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])\n",
    "# \n",
    "Ridgemodel = Ridge(alpha=.1)\n",
    "Ridgemodel.fit(X,y.reshape(-1,n_seq))\n",
    "RidgetestPredict = np.array([i for i in Ridgemodel.predict(test[:,:n_lag])])\n",
    "RidgetrainPredict = np.array([i for i in Ridgemodel.predict(train[:,:n_lag])])\n",
    "print(len(RidgetestPredict),len(RidgetrainPredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(707, 67)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RidgetrainPredict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-4e604324079d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgetrainPredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAll_Data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_card\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_p_raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RidgetrainPredict' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = RidgetrainPredict\n",
    "t_p = All_Data[val_card]['train_p_raw'][:-1,-n_seq:]\n",
    "for i in range(0,len(predictions[0][0])):\n",
    "    p = np.array([j[0][i] for j in predictions])\n",
    "    t = np.array([j[i] for j in t_p])\n",
    "    print(np.sqrt(np.mean((p-t)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
